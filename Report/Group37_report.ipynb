{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9acf31",
   "metadata": {},
   "source": [
    "# Basics of Mobile Robotics Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d701c",
   "metadata": {},
   "source": [
    "Autumn **2025-2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c6a01",
   "metadata": {},
   "source": [
    "Group n°37:\n",
    "- *Daniel Alves Ataìde - 340497*\n",
    "- *Rachid Kahia - 343266*\n",
    "- *Nicholas Thole - 356526*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6baa3a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Project Objective\n",
    "\n",
    "The objective of this project is to develop an autonomous mobile robot capable of navigating from a starting position to a goal position while avoiding obstacles in a controlled environment. The robot used is a Thymio II, equipped with infrared proximity sensors and controlled via the TDM protocol. A camera positioned above the workspace provides real-time visual feedback, enabling the robot to localize itself and detect obstacles.\n",
    "\n",
    "The system handles two types of obstacles:\n",
    "\n",
    "1. **Static obstacles (camera-detected)**: These are known obstacles placed in the environment that are detected by the overhead camera through color filtering. They are used during the global path planning phase to compute collision-free trajectories. These obstacles are not detectable by the robot's onboard sensors.\n",
    "\n",
    "2. **Dynamic/unexpected obstacles (sensor-detected)**: These are obstacles that may appear during navigation and are detected by the Thymio's infrared proximity sensors. The robot must reactively avoid these obstacles using local navigation strategies while maintaining progress toward the goal.\n",
    "\n",
    "In addition to basic navigation, the system must demonstrate robustness through two additional objectives:\n",
    "\n",
    "1. **Vision-loss resilience**: If the camera is blocked or vision is temporarily lost, the robot must continue navigating toward the goal using dead reckoning based on wheel odometry. While precision will be reduced without visual feedback, the Kalman filter's prediction step allows the robot to maintain an estimated pose and continue making progress.\n",
    "\n",
    "2. **Kidnapping recovery**: If the robot is suddenly moved to a new position (the \"kidnapping problem\"), the system must detect this significant change in pose, recompute the optimal path from the new location to the goal, and resume navigation seamlessly.\n",
    "\n",
    "### System Requirements\n",
    "\n",
    "The project must fulfill the following requirements:\n",
    "\n",
    "1. **Vision System**: Use a camera to detect the robot's position and orientation (pose), identify obstacles, and locate the goal position.\n",
    "2. **Global Path Planning**: Compute an optimal collision-free path from start to goal using a \"shortest path\" algorithm on a visibility graph.\n",
    "3. **State Estimation**: Implement an Extended Kalman Filter (EKF) to fuse visual measurements with odometry for robust pose estimation.\n",
    "4. **Motion Control**: Design a controller to follow the planned path by computing appropriate wheel velocities.\n",
    "5. **Local Obstacle Avoidance**: Use the robot's proximity sensors to reactively avoid unexpected obstacles while maintaining progress toward the goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284b5de",
   "metadata": {},
   "source": [
    "## System Overview and Architecture\n",
    "\n",
    "<img src=\"System_Overview_temporary.jpeg\" alt=\"System Overview\" width=\"80%\">\n",
    "\n",
    "The navigation system follows a sequential pipeline that combines global planning with real-time feedback control:\n",
    "\n",
    "1. **Vision System** creates a map of the environment by detecting obstacles and identifying potential waypoints positioned safely away from obstacle edges where the robot can pass. This environmental map is passed to the path planner.\n",
    "\n",
    "2. **Path Planner (Global Navigation)** receives the map and computes the shortest collision-free path from the robot's starting position to the goal using the A* algorithm on a visibility graph. The result is a sequence of waypoint coordinates that define the optimal trajectory.\n",
    "\n",
    "3. **Kalman Filter** provides accurate state estimation by fusing two sources of information: the noisy pose measurements from the camera and the imperfect position estimates derived from wheel odometry. This sensor fusion produces a filtered pose estimate that combines the strengths of both sensors while mitigating their individual weaknesses.\n",
    "\n",
    "4. **Motion Controller** receives the waypoint sequence from the path planner and the current filtered pose from the Kalman filter. It computes appropriate left and right wheel speeds to navigate the robot toward the next waypoint in the sequence.\n",
    "\n",
    "5. **Local Navigation** operates in parallel with global navigation. The Thymio continuously sends its infrared proximity sensor data to the local avoidance system, which detects unexpected obstacles not visible to the camera. When obstacles are detected, the local navigation modulates the controller's wheel speeds to reactively avoid collisions while maintaining progress toward the goal.\n",
    "\n",
    "This architecture ensures robust navigation by combining optimal global planning with reactive local obstacle avoidance.\n",
    "\n",
    "\n",
    "\n",
    "### System Components\n",
    "\n",
    "Each of the main components described above is implemented as a dedicated Python class, encapsulating all the functions necessary to fulfill its specific tasks. Note that the **Local Navigation** functions are integrated within the **Motion Controller** class, as they work closely together to control the robot's movement. Additionally, a **Visualizer** class provides real-time visualization on the computer screen, displaying the camera feed with overlays showing detected obstacles, robot position, planned path, and sensor readings.\n",
    "\n",
    "### Running the Project\n",
    "\n",
    "To run the complete navigation system, execute the [`main.py`](main.py) script. This script contains everything needed to initialize all components, establish communication with the Thymio, start the camera feed, and execute the full navigation pipeline from start to goal.\n",
    "\n",
    "**Note:** The entire project was designed and tested exclusively on macOS. While the core functionality should work on Windows, minor adjustments may be required to connect to the Thymio robot, for camera access, file paths, or system-specific dependencies.\n",
    "\n",
    "\n",
    "### Team Contributions\n",
    "\n",
    "The project was developed collaboratively, with each team member taking primary responsibility for specific components:\n",
    "\n",
    "- **Daniel Alves Ataìde**: Implemented the local navigation system using the ANN approach and developed the Kalman Filter in collaboration with Rachid. Daniel also created the overall code architecture and integrated all components, ensuring that the vision system, path planner, controller, and filters worked together seamlessly.\n",
    "\n",
    "- **Rachid Kahia**: Led the development of the vision system with contributions from the entire team, and co-developed the Kalman Filter with Daniel.\n",
    "\n",
    "- **Nicholas Thole**: Designed and implemented the global path planning algorithm and the motion controller.\n",
    "\n",
    "This report was primarily written by Rachid and Nicholas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe724a9",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472c611",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7270a15",
   "metadata": {},
   "source": [
    "A significant part of this project relies on computer vision. We made extensive use of the *OpenCV* library to implement the image-processing pipeline that enables the robot to interpret its environment. All vision-related functionalities are organized within a dedicated `VisionSystem` class, implemented in the file *vision_system.py*.\n",
    "\n",
    "The camera was placed above the setup at a slight angle. As a first step, we calibrated the image by detecting ArUco markers positioned around the environment. These markers allowed us to isolate and crop the relevant rectangular area. We then applied a perspective transform to obtain a top-down, orthonormal view of the scene, which became the reference frame for all subsequent processing. ArUco markers were also used to detect both the robot and the goal position.\n",
    "\n",
    "To detect obstacles, we initially considered using black objects combined with classical edge detection. However, the presence of other dark elements in the scene such as the ArUco markers and parts of the Thymio robot made this approach unreliable. We therefore switched to using blue obstacles and applied color filtering to extract blue regions from the image. This method turned out to be highly effective and was ultimately adopted in our final pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ae292",
   "metadata": {},
   "source": [
    "## Connecting to Thymio\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before establishing a connection in code, you must prepare the Thymio for wireless communication:\n",
    "\n",
    "1. **Launch Thymio Suite**: Open the Thymio Suite application on your computer\n",
    "2. **Detect the Robot**: Wait for your Thymio to be detected via the wireless dongle\n",
    "3. **Open Aseba Studio**: Click to open Aseba Studio from Thymio Suite\n",
    "4. **Unlock the Robot**: Click the unlock icon in Aseba Studio to make the robot available for external connections\n",
    "\n",
    "This unlocking step is crucial - without it, the TDM client cannot establish a connection to the robot.\n",
    "\n",
    "### Connection Implementation\n",
    "\n",
    "We connect to the Thymio wirelessly using the **TDM (Thymio Device Manager) protocol** through the `tdmclient` Python library. The connection is managed by a custom [`ThymioConnection`](motion_controller.py) context manager class that ensures proper resource handling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09abf7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "\n",
    "class ThymioConnection:\n",
    "    \"\"\"\n",
    "    Context manager for safe Thymio connection handling.\n",
    "    Automatically locks on connection and unlocks on exit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.client = None\n",
    "        self.node = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        # Create async client\n",
    "        self.client = ClientAsync()\n",
    "        \n",
    "        # Wait for robot to be discovered\n",
    "        self.node = aw(self.client.wait_for_node())\n",
    "        \n",
    "        # Lock the robot for exclusive access\n",
    "        aw(self.node.lock())\n",
    "        \n",
    "        print(f\"[Thymio] Connected and locked: {self.node}\")\n",
    "        return self.client, self.node\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Stop motors\n",
    "        if self.node is not None:\n",
    "            aw(self.node.set_variables({\n",
    "                \"motor.left.target\": [0],\n",
    "                \"motor.right.target\": [0],\n",
    "            }))\n",
    "        \n",
    "        # Unlock and disconnect\n",
    "        aw(self.node.stop())\n",
    "        aw(self.node.unlock())\n",
    "        self.client.close()\n",
    "        \n",
    "        print(\"[Thymio] Disconnected\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0be53",
   "metadata": {},
   "source": [
    "The connection is established at the start of the navigation loop in [`main.py`](main.py):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6231a",
   "metadata": {},
   "source": [
    "## Global Navigation\n",
    "\n",
    "The global navigation system computes an optimal collision-free path from the robot's starting position to the goal before movement begins. It uses the map of static obstacles provided by the vision system to build a **visibility graph**: a network where nodes represent potential waypoints (vertices of the expanded obstacles, start, and goal), and edges connect pairs of nodes that have an unobstructed line of sight between them. The A* algorithm then searches this graph to find the shortest path, resulting in a sequence of waypoints the robot should follow.\n",
    "\n",
    "### Path Planning Algorithm\n",
    "\n",
    "We implemented the **A\\* (A-star) algorithm** to find the shortest path through the environment. A\\* was chosen over simpler alternatives like Dijkstra's algorithm because it uses a heuristic function to guide the search toward the goal, making it significantly more efficient. While Dijkstra explores nodes uniformly in all directions, A\\* prioritizes nodes that appear closer to the goal based on Euclidean distance. This targeted exploration reduces computation time—a critical advantage for real-time robotics applications where the robot may need to replan paths quickly, such as after kidnapping detection or when encountering unexpected obstacles. Despite its efficiency gains, A\\* still guarantees finding the optimal shortest path when using an admissible heuristic.\n",
    "\n",
    "### Visibility Graph Construction\n",
    "\n",
    "The path planning operates on a **visibility graph** constructed from the obstacle map. The process works as follows:\n",
    "\n",
    "1. **Obstacle Expansion**: Each detected obstacle polygon is expanded by the robot's radius plus a safety margin. This creates a \"configuration space\" where the robot's center can safely travel without collision.\n",
    "\n",
    "2. **Waypoint Generation**: The vertices of the expanded obstacle polygons become candidate waypoints in the graph. These represent safe positions near obstacles where the robot can navigate.\n",
    "\n",
    "3. **Edge Creation**: Two waypoints are connected by an edge if a straight line between them does not intersect any obstacle. These edges represent feasible paths the robot can follow.\n",
    "\n",
    "4. **Start and Goal Integration**: The robot's current position and goal position are added as nodes and connected to all visible waypoints.\n",
    "\n",
    "### A\\* Implementation\n",
    "\n",
    "The A\\* algorithm explores this graph using the evaluation function:\n",
    "\n",
    "```\n",
    "f(n) = g(n) + h(n)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `g(n)` = actual cost from start to node `n` (cumulative path distance)\n",
    "- `h(n)` = heuristic estimate from `n` to goal (Euclidean distance)\n",
    "- `f(n)` = estimated total cost of the path through `n`\n",
    "\n",
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "```python\n",
    "def a_star(graph, start, goal):\n",
    "    \"\"\"\n",
    "    A* pathfinding algorithm on visibility graph.\n",
    "    Returns list of waypoint coordinates from start to goal.\n",
    "    \"\"\"\n",
    "    open_set = PriorityQueue()\n",
    "    open_set.put((0, start))\n",
    "    \n",
    "    came_from = {}\n",
    "    g_score = {node: float('inf') for node in graph.nodes}\n",
    "    g_score[start] = 0\n",
    "    \n",
    "    f_score = {node: float('inf') for node in graph.nodes}\n",
    "    f_score[start] = heuristic(start, goal)\n",
    "    \n",
    "    while not open_set.empty():\n",
    "        current = open_set.get()[1]\n",
    "        \n",
    "        if current == goal:\n",
    "            return reconstruct_path(came_from, current)\n",
    "        \n",
    "        for neighbor in graph.neighbors(current):\n",
    "            tentative_g = g_score[current] + distance(current, neighbor)\n",
    "            \n",
    "            if tentative_g < g_score[neighbor]:\n",
    "                came_from[neighbor] = current\n",
    "                g_score[neighbor] = tentative_g\n",
    "                f_score[neighbor] = tentative_g + heuristic(neighbor, goal)\n",
    "                open_set.put((f_score[neighbor], neighbor))\n",
    "    \n",
    "    return None  # No path found\n",
    "\n",
    "def heuristic(node, goal):\n",
    "    \"\"\"Euclidean distance heuristic.\"\"\"\n",
    "    return np.linalg.norm(np.array(node) - np.array(goal))\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "The algorithm maintains a priority queue of nodes to explore, always expanding the node with the lowest `f(n)` value first. This ensures efficient exploration while maintaining optimality.\n",
    "\n",
    "### Output\n",
    "\n",
    "The result of the path planning is a sequence of waypoint coordinates `[(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)]` that defines the optimal trajectory from start to goal. This path is then passed to the motion controller, which navigates the robot through each waypoint in sequence while the Kalman filter provides accurate pose estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b3620",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b520517e",
   "metadata": {},
   "source": [
    "## Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a88b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a033d37e",
   "metadata": {},
   "source": [
    "## Motion Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9dae9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d537f441",
   "metadata": {},
   "source": [
    "## Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0a029",
   "metadata": {},
   "source": [
    "While global path planning provides an optimal collision-free trajectory based on known obstacles detected by the camera, the robot must also handle **unexpected obstacles** that appear during navigation. These dynamic obstacles are not visible in the camera frame and cannot be anticipated during the planning phase. To address this, we implemented a reactive local navigation system using the Thymio's infrared proximity sensors.\n",
    "\n",
    "### Artificial Neural Network (ANN) Approach\n",
    "\n",
    "The local obstacle avoidance is implemented using an **Artificial Neural Network (ANN)** that runs autonomously on the Thymio at 10Hz. This approach is inspired by Braitenberg vehicles and uses a simple single-layer neural network architecture.\n",
    "\n",
    "**Architecture:**\n",
    "- **Inputs**: 7 proximity sensor readings (5 front sensors + 2 rear sensors)\n",
    "- **Outputs**: 2 motor speeds (left and right wheels)\n",
    "- **Weights**: A 2×7 weight matrix that maps sensor activations to motor commands\n",
    "\n",
    "The weight matrix is designed such that:\n",
    "- Front center sensors strongly reduce both wheel speeds (braking)\n",
    "- Front left sensors reduce right wheel speed more than left (turn right to avoid)\n",
    "- Front right sensors reduce left wheel speed more than right (turn left to avoid)\n",
    "- Rear sensors provide a slight bias to help escape tight situations\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "The ANN program is compiled and uploaded to the Thymio using the Aseba scripting language. The code runs directly on the robot's microcontroller, ensuring fast reaction times without communication delays. The behavior can be summarized as:\n",
    "\n",
    "```\n",
    "For each proximity sensor reading:\n",
    "    motor_left += sensor_value * weight_left[sensor_index]\n",
    "    motor_right += sensor_value * weight_right[sensor_index]\n",
    "```\n",
    "\n",
    "The weights were tuned experimentally to achieve smooth avoidance behavior while maintaining forward progress toward the goal.\n",
    "\n",
    "### Integration with Global Navigation\n",
    "\n",
    "The local avoidance system is activated when the maximum proximity sensor reading exceeds a threshold (typically 1500-2000 on a scale of 0-4500). When no obstacles are nearby, the robot follows motor commands from the motion controller based on the global path. When obstacles are detected, the ANN modulates these base speeds to reactively avoid collisions while still attempting to maintain the general direction toward the waypoint.\n",
    "\n",
    "This hybrid approach combines the optimality of global planning with the reactivity of local avoidance, allowing the robot to navigate complex environments with both static and dynamic obstacles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871deb4a",
   "metadata": {},
   "source": [
    "## Kidnapping Resilience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e88ed3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52c0bf31",
   "metadata": {},
   "source": [
    "## Dead Reckoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861eaee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368a3433",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240f614",
   "metadata": {},
   "source": [
    "\"blabla\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-course)",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
