{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c052075d",
   "metadata": {},
   "source": [
    "# Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5419194",
   "metadata": {},
   "source": [
    "A significant part of this project relies on computer vision. We made extensive use of the *OpenCV* library to implement the image-processing pipeline that enables the robot to interpret its environment. All vision-related functionalities are organized within a dedicated `VisionSystem` class, implemented in the file *vision_system.py*.\n",
    "\n",
    "The camera was placed above the setup at a slight angle. As a first step, we calibrated the image by detecting ArUco markers positioned around the environment. These markers allowed us to isolate and crop the relevant rectangular area. We then applied a perspective transform to obtain a top-down, orthonormal view of the scene, which became the reference frame for all subsequent processing. ArUco markers were also used to detect both the robot and the goal position.\n",
    "\n",
    "To detect obstacles, we initially considered using black objects combined with classical edge detection. However, the presence of other dark elements in the scene such as the ArUco markers and parts of the Thymio robot made this approach unreliable. We therefore switched to using blue obstacles and applied color filtering to extract blue regions from the image. This method turned out to be highly effective and was ultimately adopted in our final pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800e69e",
   "metadata": {},
   "source": [
    "METTRE PHOTO DU SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1676641",
   "metadata": {},
   "source": [
    "At the beginning of the project, we planned to use the provided Aukey webcam. However, once we started coding and sharing our programs, we quickly ran into practical issues since we only had a single webcam. As we all work on Mac, a much more efficient solution was to use our iPhones as cameras instead. Thanks to the Apple ecosystem, each of us could easily use our iPhone camera as an external webcam for our MacBooks. This gave us higher image quality and allowed everyone to run and test the vision code independently, without relying on the one shared webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b29fa1",
   "metadata": {},
   "source": [
    "We began by implementing the `VisionSystem` constructor, which initializes all required variables. It opens the camera stream using OpenCV’s `VideoCapture` and loads the ArUco dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cv2 import aruco\n",
    "\n",
    "class VisionSystem:\n",
    "\n",
    "    def __init__(self, camera_id=0, aruco_dict_type=aruco.DICT_4X4_50):\n",
    "        self.cap = cv2.VideoCapture(camera_id)\n",
    "        self.aruco_dict = aruco.getPredefinedDictionary(aruco_dict_type)\n",
    "        self.aruco_params = aruco.DetectorParameters()\n",
    "\n",
    "        # Initialize variables\n",
    "        self.detected_markers = {}\n",
    "        self.corners = None  # [(x1, y1), ...]\n",
    "        self.transform_matrix = None\n",
    "        self.mm2px = None\n",
    "        self.map_size = (900, 600)\n",
    "        self.goal_position = None  # (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254846f",
   "metadata": {},
   "source": [
    "The core of the setup is the `calibrate` function, which reads frames from the camera and detects all relevant ArUco markers. To differentiate their roles, we pass two parameters: `corner_ids` for the four map corners and `goal_id` for the goal marker.\n",
    "Once all four corner markers are detected, we order them, draw their positions in green on the displayed image, and connect them with green lines to visualize the rectangular map region that will be extracted. We also highlight the goal marker with a circle and label it “GOAL”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba647dea",
   "metadata": {},
   "source": [
    "METTRE SCREEN DES LIGNES VERTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01fb4c",
   "metadata": {},
   "source": [
    "When the user is satisfied with the setup and marker detection, pressing the “c” key completes the calibration. This triggers the perspective transform and computes the millimeter-to-pixel scaling factor used throughout the rest of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0dee4",
   "metadata": {},
   "source": [
    "METTRE SCREEN DE L’AFFICHAGE APRES C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3669f",
   "metadata": {},
   "source": [
    "### Converting Real-World Distances (mm) to Pixels\n",
    "\n",
    "During calibration, the physical height of the map is known:\n",
    "\n",
    "$H_{\\text{real}}$ (in mm)\n",
    "\n",
    "and the corresponding height in the transformed top-down image is fixed:\n",
    "\n",
    "$H_{\\text{px}}$ (in pixels)\n",
    "\n",
    "The conversion factor from millimeters to pixels is therefore:\n",
    "$$ \\text{mm2px} = \\frac{H_{\\text{px}}}{H_{\\text{real}}} $$\n",
    "\n",
    "This scalar tells us how many pixels correspond to one millimeter in the real world.  \n",
    "It allows converting any real-world measurement $d_{\\text{mm}}$ into pixel units:\n",
    "$$ d_{\\text{px}} = d_{\\text{mm}} \\cdot \\text{mm2px} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec00739",
   "metadata": {},
   "source": [
    "### Robot pose detection\n",
    "\n",
    "We use the function `detect_raw_robot_pose` to detect the robot pose, and get its orientation. To do this, we use trigonometry. Here is the thought process behind the robot orientation estimation from the aruco marker :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_robot_raw_pose(self, frame):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        np.array: [x, y, theta] in map coordinates, or None if not detected\n",
    "    \"\"\"\n",
    "\n",
    "    marker_centers = self._detect_marker_centers(frame, target_ids={4})\n",
    "\n",
    "    if 4 not in marker_centers:\n",
    "        print(\"Robot marker not detected!\")\n",
    "        return None\n",
    "\n",
    "    robot_data = self.detected_markers[4]\n",
    "\n",
    "    corners = robot_data['corners'] # Orientation, corners are: TL, TR, BR, BL\n",
    "\n",
    "    tl = corners[0]\n",
    "    tr = corners[1]\n",
    "\n",
    "    top_center = (tl + tr) / 2\n",
    "    center = robot_data['center']\n",
    "    x, y = center\n",
    "\n",
    "    dx = top_center[0] - center[0]\n",
    "    dy = top_center[1] - center[1]\n",
    "\n",
    "    theta = np.arctan2(dy, dx) # Compute angle in radians\n",
    "\n",
    "    return np.array([x, y, theta])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e1255",
   "metadata": {},
   "source": [
    "To compute the robot’s orientation, we use the top edge of the ArUco marker as a reference direction.\n",
    "\n",
    "Given the marker corners TL and TR, and the marker center c, we define:\n",
    "\n",
    "$$\n",
    "m = \\frac{TL + TR}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v = m - c\n",
    "$$\n",
    "\n",
    "Component-wise:\n",
    "\n",
    "$$\n",
    "v_x = m_x - c_x, \\qquad v_y = m_y - c_y\n",
    "$$\n",
    "\n",
    "The robot’s orientation angle is then obtained using:\n",
    "\n",
    "$$\n",
    "\\theta = \\operatorname{atan2}(v_y, v_x)\n",
    "$$\n",
    "\n",
    "This angle corresponds to the direction from the center of the marker toward the top edge, expressed in radians.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d48947",
   "metadata": {},
   "source": [
    "### Obstacle detection and Image processing\n",
    "To detect obstacles, we first capture frames from the camera and apply a color filter using OpenCV’s `inRange` function (with a lower and upper threshold for blue) on the HSV representation of the image. This produces a mask, isolating only the blue pixels corresponding to our obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9ef2e",
   "metadata": {},
   "source": [
    "METTRE PHOTO DU MASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be896b",
   "metadata": {},
   "source": [
    "Next, we apply a 3×3 Gaussian blur on the mask to reduce noise, followed by Canny edge detection to extract the contours of the polygonal obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e2838",
   "metadata": {},
   "source": [
    "METTRE SCREEN DU CANNY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bf186",
   "metadata": {},
   "source": [
    "With this edge image, we scale each obstacle’s contour using our custom `scale_contour` function, and then use the very powerful OpenCV functions `findContours `and `approxPolyDP` in our `detect_contours` function to recover the vertices of the scaled shapes. To avoid false positives, we restrict the obstacles to polygons with exactly four vertices.\n",
    "\n",
    "Finally, we draw the scaled polygons in red, along with red circles indicating each detected vertex, on the output frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88a76a",
   "metadata": {},
   "source": [
    "METTRE SCREEN DES SCALED POLYGONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f51c20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BOMR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
