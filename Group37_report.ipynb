{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9acf31",
   "metadata": {},
   "source": [
    "# Basics of Mobile Robotics Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d701c",
   "metadata": {},
   "source": [
    "Autumn **2025-2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c6a01",
   "metadata": {},
   "source": [
    "Group n°37:\n",
    "- *Daniel Alves Ataìde - 340497*\n",
    "- *Rachid Kahia - 343266*\n",
    "- *Nicholas Thole - 356526*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6baa3a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Project Objective\n",
    "\n",
    "The objective of this project is to develop an autonomous mobile robot capable of navigating from a starting position to a goal position while avoiding obstacles in a controlled environment. The robot used is a Thymio II, equipped with infrared proximity sensors and controlled via the TDM protocol. A camera positioned above the workspace provides real-time visual feedback, enabling the robot to localize itself and detect obstacles.\n",
    "\n",
    "The system handles two types of obstacles:\n",
    "\n",
    "1. **Static obstacles (camera-detected)**: These are known obstacles placed in the environment that are detected by the overhead camera through color filtering. They are used during the global path planning phase to compute collision-free trajectories. These obstacles are not detectable by the robot's onboard sensors.\n",
    "\n",
    "2. **Dynamic/unexpected obstacles (sensor-detected)**: These are obstacles that may appear during navigation and are detected by the Thymio's infrared proximity sensors. The robot must reactively avoid these obstacles using local navigation strategies while maintaining progress toward the goal.\n",
    "\n",
    "In addition to basic navigation, the system must demonstrate robustness through two additional objectives:\n",
    "\n",
    "1. **Vision-loss resilience**: If the camera is blocked or vision is temporarily lost, the robot must continue navigating toward the goal using dead reckoning based on wheel odometry. While precision will be reduced without visual feedback, the Kalman filter's prediction step allows the robot to maintain an estimated pose and continue making progress.\n",
    "\n",
    "2. **Kidnapping recovery**: If the robot is suddenly moved to a new position (the \"kidnapping problem\"), the system must detect this significant change in pose, recompute the optimal path from the new location to the goal, and resume navigation seamlessly.\n",
    "\n",
    "### System Requirements\n",
    "\n",
    "The project must fulfill the following requirements:\n",
    "\n",
    "1. **Vision System**: Use a camera to detect the robot's position and orientation (pose), identify obstacles, and locate the goal position.\n",
    "2. **Global Path Planning**: Compute an optimal collision-free path from start to goal using a \"shortest path\" algorithm on a visibility graph.\n",
    "3. **State Estimation**: Implement an Extended Kalman Filter (EKF) to fuse visual measurements with odometry for robust pose estimation.\n",
    "4. **Motion Control**: Design a controller to follow the planned path by computing appropriate wheel velocities.\n",
    "5. **Local Obstacle Avoidance**: Use the robot's proximity sensors to reactively avoid unexpected obstacles while maintaining progress toward the goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284b5de",
   "metadata": {},
   "source": [
    "## System Overview and Architecture\n",
    "\n",
    "<img src=\"System_Overview_temporary.jpeg\" alt=\"System Overview\" width=\"80%\">\n",
    "\n",
    "The navigation system follows a sequential pipeline that combines global planning with real-time feedback control:\n",
    "\n",
    "1. **Vision System** creates a map of the environment by detecting obstacles and identifying potential waypoints positioned safely away from obstacle edges where the robot can pass. This environmental map is passed to the path planner.\n",
    "\n",
    "2. **Path Planner (Global Navigation)** receives the map and computes the shortest collision-free path from the robot's starting position to the goal using the A* algorithm on a visibility graph. The result is a sequence of waypoint coordinates that define the optimal trajectory.\n",
    "\n",
    "3. **Kalman Filter** provides accurate state estimation by fusing two sources of information: the noisy pose measurements from the camera and the imperfect position estimates derived from wheel odometry. This sensor fusion produces a filtered pose estimate that combines the strengths of both sensors while mitigating their individual weaknesses.\n",
    "\n",
    "4. **Motion Controller** receives the waypoint sequence from the path planner and the current filtered pose from the Kalman filter. It computes appropriate left and right wheel speeds to navigate the robot toward the next waypoint in the sequence.\n",
    "\n",
    "5. **Local Navigation** operates in parallel with global navigation. The Thymio continuously sends its infrared proximity sensor data to the local avoidance system, which detects unexpected obstacles not visible to the camera. When obstacles are detected, the local navigation modulates the controller's wheel speeds to reactively avoid collisions while maintaining progress toward the goal.\n",
    "\n",
    "This architecture ensures robust navigation by combining optimal global planning with reactive local obstacle avoidance.\n",
    "\n",
    "\n",
    "\n",
    "### System Components\n",
    "\n",
    "Each of the main components described above is implemented as a dedicated Python class, encapsulating all the functions necessary to fulfill its specific tasks. Note that the **Local Navigation** functions are integrated within the **Motion Controller** class, as they work closely together to control the robot's movement. Additionally, a **Visualizer** class provides real-time visualization on the computer screen, displaying the camera feed with overlays showing detected obstacles, robot position, planned path, and sensor readings.\n",
    "\n",
    "### Running the Project\n",
    "\n",
    "To run the complete navigation system, execute the [`main.py`](main.py) script. This script contains everything needed to initialize all components, establish communication with the Thymio, start the camera feed, and execute the full navigation pipeline from start to goal.\n",
    "\n",
    "**Note:** The entire project was designed and tested exclusively on macOS. While the core functionality should work on Windows, minor adjustments may be required to connect to the Thymio robot, for camera access, file paths, or system-specific dependencies.\n",
    "\n",
    "\n",
    "### Team Contributions\n",
    "\n",
    "The project was developed collaboratively, with each team member taking primary responsibility for specific components:\n",
    "\n",
    "- **Daniel Alves Ataìde**: Implemented the local navigation system using the ANN approach and developed the Kalman Filter in collaboration with Rachid. Daniel also created the overall code architecture and integrated all components, ensuring that the vision system, path planner, controller, and filters worked together seamlessly.\n",
    "\n",
    "- **Rachid Kahia**: Led the development of the vision system with contributions from the entire team, and co-developed the Kalman Filter with Daniel.\n",
    "\n",
    "- **Nicholas Thole**: Designed and implemented the global path planning algorithm and the motion controller.\n",
    "\n",
    "This report was primarily written by Rachid and Nicholas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe724a9",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472c611",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7270a15",
   "metadata": {},
   "source": [
    "A significant part of this project relies on computer vision. We made extensive use of the *OpenCV* library to implement the image-processing pipeline that enables the robot to interpret its environment. All vision-related functionalities are organized within a dedicated `VisionSystem` class, implemented in the file *vision_system.py*.\n",
    "\n",
    "The camera was placed above the setup at a slight angle. As a first step, we calibrated the image by detecting ArUco markers positioned around the environment. These markers allowed us to isolate and crop the relevant rectangular area. We then applied a perspective transform to obtain a top-down, orthonormal view of the scene, which became the reference frame for all subsequent processing. ArUco markers were also used to detect both the robot and the goal position.\n",
    "\n",
    "To detect obstacles, we initially considered using black objects combined with classical edge detection. However, the presence of other dark elements in the scene such as the ArUco markers and parts of the Thymio robot made this approach unreliable. We therefore switched to using blue obstacles and applied color filtering to extract blue regions from the image. This method turned out to be highly effective and was ultimately adopted in our final pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ae292",
   "metadata": {},
   "source": [
    "## Connecting to Thymio\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before establishing a connection in code, you must prepare the Thymio for wireless communication:\n",
    "\n",
    "1. **Launch Thymio Suite**: Open the Thymio Suite application on your computer\n",
    "2. **Detect the Robot**: Wait for your Thymio to be detected via the wireless dongle\n",
    "3. **Open Aseba Studio**: Click to open Aseba Studio from Thymio Suite\n",
    "4. **Unlock the Robot**: Click the unlock icon in Aseba Studio to make the robot available for external connections\n",
    "\n",
    "This unlocking step is crucial - without it, the TDM client cannot establish a connection to the robot.\n",
    "\n",
    "### Connection Implementation\n",
    "\n",
    "We connect to the Thymio wirelessly using the **TDM (Thymio Device Manager) protocol** through the `tdmclient` Python library. The connection is managed by a custom [`ThymioConnection`](motion_controller.py) context manager class that ensures proper resource handling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6785fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "\n",
    "class ThymioConnection:\n",
    "    \"\"\"\n",
    "    Context manager for safe Thymio connection handling.\n",
    "    Automatically locks on connection and unlocks on exit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.client = None\n",
    "        self.node = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        # Create async client\n",
    "        self.client = ClientAsync()\n",
    "        \n",
    "        # Wait for robot to be discovered\n",
    "        self.node = aw(self.client.wait_for_node())\n",
    "        \n",
    "        # Lock the robot for exclusive access\n",
    "        aw(self.node.lock())\n",
    "        \n",
    "        print(f\"[Thymio] Connected and locked: {self.node}\")\n",
    "        return self.client, self.node\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Stop motors\n",
    "        if self.node is not None:\n",
    "            aw(self.node.set_variables({\n",
    "                \"motor.left.target\": [0],\n",
    "                \"motor.right.target\": [0],\n",
    "            }))\n",
    "        \n",
    "        # Unlock and disconnect\n",
    "        aw(self.node.stop())\n",
    "        aw(self.node.unlock())\n",
    "        self.client.close()\n",
    "        \n",
    "        print(\"[Thymio] Disconnected\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0be53",
   "metadata": {},
   "source": [
    "The connection is established at the start of the navigation loop in [`main.py`](main.py):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6231a",
   "metadata": {},
   "source": [
    "## Global Navigation\n",
    "\n",
    "The global navigation system computes an optimal collision-free path from the robot's starting position to the goal before movement begins. It uses the map of static obstacles provided by the vision system to build a **visibility graph**: a network where nodes represent potential waypoints (vertices of the expanded obstacles, start, and goal), and edges connect pairs of nodes that have an unobstructed line of sight between them. The A* algorithm then searches this graph to find the shortest path, resulting in a sequence of waypoints the robot should follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5cc89",
   "metadata": {},
   "source": [
    "### Visibility Graph Construction\n",
    "\n",
    "Before constructing the visibility graph, the system first assembles a complete list of all nodes (waypoints) that will form the vertices of the graph. This node list is created by combining:\n",
    "\n",
    "1. **Start position**: The robot's current pose (index 0 in the node list)\n",
    "2. **Goal position**: The target destination (index 1 in the node list)  \n",
    "3. **Expanded obstacle vertices**: All corner points of the obstacles after they have been expanded by the robot's radius to account for its physical size\n",
    "\n",
    "This node assembly happens in the [`compute_path`](path_planner.py) method before calling the visibility graph builder:\n",
    "\n",
    "```python\n",
    "nodes = [np.array(start), np.array(goal)]\n",
    "for poly in obstacles:\n",
    "    for v in poly:\n",
    "        nodes.append(np.array(v))\n",
    "```\n",
    "\n",
    "By placing start and goal at indices 0 and 1 respectively, the pathfinding algorithm knows exactly which nodes represent the start and end points without needing to search through the entire list.\n",
    "\n",
    "Once the waypoint nodes are established, the system constructs a **visibility graph** that defines which waypoints can be directly connected by straight-line paths.\n",
    "\n",
    "The visibility graph is built by the [`build_visibility_graph`](path_planner.py) method, which operates as follows:\n",
    "\n",
    "1. **Graph Initialization**: An empty adjacency list is created for each node in the node list.\n",
    "\n",
    "2. **Pairwise Visibility Testing**: For every unique pair of nodes (using nested loops where j starts at i+1 to avoid duplicates), the system checks if a straight line between them would intersect any obstacle edge. This is done using the [`_visible`](path_planner.py) method.\n",
    "\n",
    "3. **Edge Creation**: If two nodes are mutually visible (no obstacle intersection), an edge is added to the graph **in both directions** with a weight equal to the Euclidean distance between them.\n",
    "\n",
    "4. **Special Case Handling**: The visibility test includes logic to prevent creating edges that cut through the interior of obstacles (diagonal connections across non-adjacent vertices of the same polygon are rejected).\n",
    "\n",
    "The implementation in [`path_planner.py`](path_planner.py) demonstrates this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c30cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visibility_graph(self, nodes, obstacles):\n",
    "    \"\"\"\n",
    "    Constructs a visibility graph from waypoints.\n",
    "    \n",
    "    Args:\n",
    "        nodes: list of np.array([x,y]) - waypoint coordinates\n",
    "        obstacles: list of polygons (obstacle boundaries)\n",
    "    \n",
    "    Returns:\n",
    "        graph: dict where graph[i] = list of (j, distance_ij)\n",
    "    \"\"\"\n",
    "    n = len(nodes)\n",
    "    graph = {i: [] for i in range(n)}\n",
    "    \n",
    "    # Test visibility between all pairs of nodes\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if self._visible(i, j, nodes, obstacles):\n",
    "                dist = np.linalg.norm(nodes[i] - nodes[j])\n",
    "                \n",
    "                # Add bidirectional edge\n",
    "                graph[i].append((j, dist))\n",
    "                graph[j].append((i, dist))\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def _visible(self, i, j, nodes, obstacles):\n",
    "    \"\"\"\n",
    "    Returns True if the line segment between nodes[i] and nodes[j]\n",
    "    does not intersect any obstacle edge.\n",
    "    \"\"\"\n",
    "    p1 = nodes[i]\n",
    "    p2 = nodes[j]\n",
    "    \n",
    "    # Check if segment crosses any obstacle boundary\n",
    "    for polygon in obstacles:\n",
    "        n = len(polygon)\n",
    "        for k in range(n):\n",
    "            q1 = polygon[k]\n",
    "            q2 = polygon[(k + 1) % n]  # wrap around\n",
    "            \n",
    "            # Skip if segment shares a vertex with this edge\n",
    "            if (np.allclose(p1, q1) or np.allclose(p1, q2) or\n",
    "                np.allclose(p2, q1) or np.allclose(p2, q2)):\n",
    "                continue\n",
    "            \n",
    "            # Check for intersection\n",
    "            if self._segments_intersect(p1, p2, q1, q2):\n",
    "                return False\n",
    "    \n",
    "    return True  # No intersections found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c827f",
   "metadata": {},
   "source": [
    "ADD A BIT OF CODE ABOVE TO BE ABLE TO RUN THIS SUCH THAT IT PRODUCES A VISIBILITY GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f746195",
   "metadata": {},
   "source": [
    "### Path Planning Algorithm\n",
    "\n",
    "We implemented the **A\\* (A-star) algorithm** to find the shortest path through the environment. A\\* was chosen over simpler alternatives like Dijkstra's algorithm because it uses a heuristic function to guide the search toward the goal, making it more efficient. While Dijkstra explores nodes uniformly in all directions, A\\* prioritizes nodes that appear closer to the goal based on Euclidean distance. This targeted exploration reduces computation time—a critical advantage for real-time robotics applications where the robot may need to replan paths quickly, such as after kidnapping detection. Despite its efficiency gains, A\\* still guarantees finding the optimal shortest path when using an admissible heuristic.\n",
    "\n",
    "In our specific use case with only two quadrilateral obstacles (resulting in approximately 10 nodes total including start and goal), the computational difference between A\\* and Dijkstra's algorithm is negligible. However, A\\* was chosen for its scalability and to demonstrate best practices for path planning in more complex environments with numerous obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310f40b",
   "metadata": {},
   "source": [
    "ADD CODE HERE TO SHOW THE SHORTEST PATH WHICH IS CALCULATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c7a71",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The result of the path planning is a sequence of waypoint coordinates `[(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)]` that defines the optimal trajectory from start to goal. This path is then passed to the motion controller, which navigates the robot through each waypoint in sequence while the Kalman filter provides accurate pose estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520517e",
   "metadata": {},
   "source": [
    "## Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a88b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a033d37e",
   "metadata": {},
   "source": [
    "## Motion Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9dae9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d537f441",
   "metadata": {},
   "source": [
    "## Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0a029",
   "metadata": {},
   "source": [
    "While global path planning provides an optimal collision-free trajectory based on known obstacles detected by the camera, the robot must also handle **unexpected obstacles** that appear during navigation. These dynamic obstacles are not visible in the camera frame and cannot be anticipated during the planning phase. To address this, we implemented a reactive local navigation system using the Thymio's infrared proximity sensors.\n",
    "\n",
    "### Artificial Neural Network (ANN) Approach\n",
    "\n",
    "The local obstacle avoidance is implemented using an **Artificial Neural Network (ANN)** that runs autonomously on the Thymio at 10Hz. This approach is inspired by Braitenberg vehicles and uses a simple single-layer neural network architecture.\n",
    "\n",
    "**Architecture:**\n",
    "- **Inputs**: 7 proximity sensor readings (5 front sensors + 2 rear sensors)\n",
    "- **Outputs**: 2 motor speeds (left and right wheels)\n",
    "- **Weights**: A 2×7 weight matrix that maps sensor activations to motor commands\n",
    "\n",
    "The weight matrix is designed such that:\n",
    "- Front center sensors strongly reduce both wheel speeds (braking)\n",
    "- Front left sensors reduce right wheel speed more than left (turn right to avoid)\n",
    "- Front right sensors reduce left wheel speed more than right (turn left to avoid)\n",
    "- Rear sensors provide a slight bias to help escape tight situations\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "The ANN program is compiled and uploaded to the Thymio using the Aseba scripting language. The code runs directly on the robot's microcontroller, ensuring fast reaction times without communication delays. The behavior can be summarized as:\n",
    "\n",
    "```\n",
    "For each proximity sensor reading:\n",
    "    motor_left += sensor_value * weight_left[sensor_index]\n",
    "    motor_right += sensor_value * weight_right[sensor_index]\n",
    "```\n",
    "\n",
    "The weights were tuned experimentally to achieve smooth avoidance behavior while maintaining forward progress toward the goal.\n",
    "\n",
    "### Integration with Global Navigation\n",
    "\n",
    "The local avoidance system is activated when the maximum proximity sensor reading exceeds a threshold (typically 1500-2000 on a scale of 0-4500). When no obstacles are nearby, the robot follows motor commands from the motion controller based on the global path. When obstacles are detected, the ANN modulates these base speeds to reactively avoid collisions while still attempting to maintain the general direction toward the waypoint.\n",
    "\n",
    "This hybrid approach combines the optimality of global planning with the reactivity of local avoidance, allowing the robot to navigate complex environments with both static and dynamic obstacles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871deb4a",
   "metadata": {},
   "source": [
    "## Kidnapping Resilience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e88ed3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52c0bf31",
   "metadata": {},
   "source": [
    "## Dead Reckoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861eaee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368a3433",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240f614",
   "metadata": {},
   "source": [
    "\"blabla\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-course)",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
