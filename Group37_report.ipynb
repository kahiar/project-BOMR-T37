{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9acf31",
   "metadata": {},
   "source": [
    "# Basics of Mobile Robotics Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d701c",
   "metadata": {},
   "source": [
    "Autumn **2025-2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c6a01",
   "metadata": {},
   "source": [
    "Group n°37:\n",
    "- *Daniel Alves Ataìde - 340497*\n",
    "- *Rachid Kahia - 343266*\n",
    "- *Nicholas Thole - 356526*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bd02b",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb9801",
   "metadata": {},
   "source": [
    "Explain the objective of the project and all of the requirements.\n",
    "\n",
    "Explain a global overview of how everything will come together -- include graphics of interactions between the 5 different components of the project.\n",
    "\n",
    "Explain which member did what part.\n",
    "\n",
    "Mention that we have a separate .py file which can run the whole project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe724a9",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7270a15",
   "metadata": {},
   "source": [
    "A significant part of this project relies on computer vision. We made extensive use of the *OpenCV* library to implement the image-processing pipeline that enables the robot to interpret its environment. All vision-related functionalities are organized within a dedicated `VisionSystem` class, implemented in the file *vision_system.py*.\n",
    "\n",
    "The camera was placed above the setup at a slight angle. As a first step, we calibrated the image by detecting ArUco markers positioned around the environment. These markers allowed us to isolate and crop the relevant rectangular area. We then applied a perspective transform to obtain a top-down, orthonormal view of the scene, which became the reference frame for all subsequent processing. ArUco markers were also used to detect both the robot and the goal position.\n",
    "\n",
    "To detect obstacles, we initially considered using black objects combined with classical edge detection. However, the presence of other dark elements in the scene such as the ArUco markers and parts of the Thymio robot made this approach unreliable. We therefore switched to using blue obstacles and applied color filtering to extract blue regions from the image. This method turned out to be highly effective and was ultimately adopted in our final pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6231a",
   "metadata": {},
   "source": [
    "## Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b3620",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b520517e",
   "metadata": {},
   "source": [
    "## Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a88b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a033d37e",
   "metadata": {},
   "source": [
    "## Motion Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9dae9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d537f441",
   "metadata": {},
   "source": [
    "## Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63646a6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368a3433",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240f614",
   "metadata": {},
   "source": [
    "\"blabla\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
