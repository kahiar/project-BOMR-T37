{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9acf31",
   "metadata": {},
   "source": [
    "# Basics of Mobile Robotics Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d701c",
   "metadata": {},
   "source": [
    "Autumn **2025-2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c6a01",
   "metadata": {},
   "source": [
    "Group n°37:\n",
    "- *Daniel Alves Ataìde - 340497*\n",
    "- *Rachid Kahia - 343266*\n",
    "- *Nicholas Thole - 356526*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6baa3a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Project Objective\n",
    "\n",
    "The objective of this project is to develop an autonomous mobile robot capable of navigating from a starting position to a goal position while avoiding obstacles in a controlled environment. The robot used is a Thymio II, equipped with infrared proximity sensors and controlled via the TDM protocol. A camera positioned above the workspace provides real-time visual feedback, enabling the robot to localize itself and detect obstacles.\n",
    "\n",
    "The system handles two types of obstacles:\n",
    "\n",
    "1. **Static obstacles (camera-detected)**: These are known obstacles placed in the environment that are detected by the overhead camera through color filtering. They are used during the global path planning phase to compute collision-free trajectories. These obstacles are not detectable by the robot's onboard sensors.\n",
    "\n",
    "2. **Dynamic/unexpected obstacles (sensor-detected)**: These are obstacles that may appear during navigation and are detected by the Thymio's infrared proximity sensors. The robot must reactively avoid these obstacles using local navigation strategies while maintaining progress toward the goal.\n",
    "\n",
    "In addition to basic navigation, the system must demonstrate robustness through two additional objectives:\n",
    "\n",
    "1. **Vision-loss resilience**: If the camera is blocked or vision is temporarily lost, the robot must continue navigating toward the goal using dead reckoning based on wheel odometry. While precision will be reduced without visual feedback, the Kalman filter's prediction step allows the robot to maintain an estimated pose and continue making progress.\n",
    "\n",
    "2. **Kidnapping recovery**: If the robot is suddenly moved to a new position (the \"kidnapping problem\"), the system must detect this significant change in pose, recompute the optimal path from the new location to the goal, and resume navigation seamlessly.\n",
    "\n",
    "### System Requirements\n",
    "\n",
    "The project must fulfill the following requirements:\n",
    "\n",
    "1. **Vision System**: Use a camera to detect the robot's position and orientation (pose), identify obstacles, and locate the goal position.\n",
    "2. **Global Path Planning**: Compute an optimal collision-free path from start to goal using a \"shortest path\" algorithm on a visibility graph.\n",
    "3. **State Estimation**: Implement an Extended Kalman Filter (EKF) to fuse visual measurements with odometry for robust pose estimation.\n",
    "4. **Motion Control**: Design a controller to follow the planned path by computing appropriate wheel velocities.\n",
    "5. **Local Obstacle Avoidance**: Use the robot's proximity sensors to reactively avoid unexpected obstacles while maintaining progress toward the goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284b5de",
   "metadata": {},
   "source": [
    "## System Overview and Architecture\n",
    "\n",
    "<img src=\"Report/System_Overview_temporary.jpeg\" alt=\"System Overview\" width=\"60%\">\n",
    "\n",
    "The navigation system follows a sequential pipeline that combines global planning with real-time feedback control:\n",
    "\n",
    "1. **Vision System** creates a map of the environment by detecting obstacles and identifying potential waypoints positioned safely away from obstacle edges where the robot can pass. This environmental map is passed to the path planner.\n",
    "\n",
    "2. **Path Planner (Global Navigation)** receives the map and computes the shortest collision-free path from the robot's starting position to the goal using the A* algorithm on a visibility graph. The result is a sequence of waypoint coordinates that define the optimal trajectory.\n",
    "\n",
    "3. **Kalman Filter** provides accurate state estimation by fusing two sources of information: the noisy pose measurements from the camera and the imperfect position estimates derived from wheel odometry. This sensor fusion produces a filtered pose estimate that combines the strengths of both sensors while mitigating their individual weaknesses.\n",
    "\n",
    "4. **Motion Controller** receives the waypoint sequence from the path planner and the current filtered pose from the Kalman filter. It computes appropriate left and right wheel speeds to navigate the robot toward the next waypoint in the sequence.\n",
    "\n",
    "5. **Local Navigation** operates in parallel with global navigation. The Thymio continuously sends its infrared proximity sensor data to the local avoidance system, which detects unexpected obstacles not visible to the camera. When obstacles are detected, the local navigation modulates the controller's wheel speeds to reactively avoid collisions while maintaining progress toward the goal.\n",
    "\n",
    "This architecture ensures robust navigation by combining optimal global planning with reactive local obstacle avoidance.\n",
    "\n",
    "\n",
    "\n",
    "### System Components\n",
    "\n",
    "Each of the main components described above is implemented as a dedicated Python class, encapsulating all the functions necessary to fulfill its specific tasks. Note that the **Local Navigation** functions are integrated within the **Motion Controller** class, as they work closely together to control the robot's movement. Additionally, a **Visualizer** class provides real-time visualization on the computer screen, displaying the camera feed with overlays showing detected obstacles, robot position, planned path, and sensor readings.\n",
    "\n",
    "The system also includes a [`utils.py`](utils.py) module that contains important constants used throughout the codebase, such as the Thymio's physical dimensions (wheel base width, wheel radius).$^{[1.1]}$\n",
    "\n",
    "### Running the Project\n",
    "\n",
    "To run the complete navigation system, execute the [`main.py`](main.py) script. This script contains everything needed to initialize all components, establish communication with the Thymio, start the camera feed, and execute the full navigation pipeline from start to goal.\n",
    "\n",
    "**Note:** The entire project was designed and tested exclusively on macOS. While the core functionality should work on Windows, minor adjustments may be required to connect to the Thymio robot, for camera access, file paths, or system-specific dependencies.\n",
    "\n",
    "\n",
    "### Team Contributions\n",
    "\n",
    "The project was developed collaboratively, with each team member taking primary responsibility for specific components:\n",
    "\n",
    "- **Daniel Alves Ataìde**: Implemented the local navigation system using the ANN approach and developed the Kalman Filter in collaboration with Rachid. Daniel also created the overall code architecture and integrated all components, ensuring that the vision system, path planner, controller, and filters worked together seamlessly.\n",
    "\n",
    "- **Rachid Kahia**: Led the development of the vision system with contributions from the entire team, and co-developed the Kalman Filter with Daniel.\n",
    "\n",
    "- **Nicholas Thole**: Designed and implemented the global path planning algorithm and the motion controller.\n",
    "\n",
    "This report was primarily written by Rachid and Nicholas.\n",
    "\n",
    "---\n",
    "\n",
    "**Note on Citations:** Throughout this report, sources are cited using bracketed numbers (e.g. $^{[1.1]}$). All referenced sources can be found in the Bibliography section at the end of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe724a9",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7270a15",
   "metadata": {},
   "source": [
    "A significant part of this project relies on computer vision. We made extensive use of the *OpenCV* library to implement the image-processing pipeline that enables the robot to interpret its environment. All vision-related functionalities are organized within a dedicated `VisionSystem` class, implemented in the file *vision_system.py*.\n",
    "\n",
    "The camera was placed above the setup at a slight angle. As a first step, we calibrated the image by detecting ArUco markers positioned around the environment. These markers allowed us to isolate and crop the relevant rectangular area. We then applied a perspective transform to obtain a top-down, orthonormal view of the scene, which became the reference frame for all subsequent processing. ArUco markers were also used to detect both the robot and the goal position.\n",
    "\n",
    "To detect obstacles, we initially considered using black objects combined with classical edge detection. However, the presence of other dark elements in the scene such as the ArUco markers and parts of the Thymio robot made this approach unreliable. We therefore switched to using blue obstacles and applied color filtering to extract blue regions from the image. This method turned out to be highly effective and was ultimately adopted in our final pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ae292",
   "metadata": {},
   "source": [
    "## Connecting to Thymio\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before establishing a connection in code, you must prepare the Thymio for wireless communication:\n",
    "\n",
    "1. **Launch Thymio Suite**: Open the Thymio Suite application on your computer\n",
    "2. **Detect the Robot**: Wait for your Thymio to be detected via the wireless dongle\n",
    "3. **Open Aseba Studio**: Click to open Aseba Studio from Thymio Suite\n",
    "4. **Unlock the Robot**: Click the unlock icon in Aseba Studio to make the robot available for external connections\n",
    "\n",
    "This unlocking step is crucial - without it, the TDM client cannot establish a connection to the robot.\n",
    "\n",
    "### Connection Implementation\n",
    "\n",
    "We connect to the Thymio wirelessly using the **TDM (Thymio Device Manager) protocol** through the `tdmclient` Python library. The connection is managed by a custom [`ThymioConnection`](motion_controller.py) context manager class that ensures proper resource handling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6785fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "\n",
    "class ThymioConnection:\n",
    "    \"\"\"\n",
    "    Context manager for safe Thymio connection handling.\n",
    "    Automatically locks on connection and unlocks on exit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.client = None\n",
    "        self.node = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        # Create async client\n",
    "        self.client = ClientAsync()\n",
    "        \n",
    "        # Wait for robot to be discovered\n",
    "        self.node = aw(self.client.wait_for_node())\n",
    "        \n",
    "        # Lock the robot for exclusive access\n",
    "        aw(self.node.lock())\n",
    "        \n",
    "        print(f\"[Thymio] Connected and locked: {self.node}\")\n",
    "        return self.client, self.node\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Stop motors\n",
    "        if self.node is not None:\n",
    "            aw(self.node.set_variables({\n",
    "                \"motor.left.target\": [0],\n",
    "                \"motor.right.target\": [0],\n",
    "            }))\n",
    "        \n",
    "        # Unlock and disconnect\n",
    "        aw(self.node.stop())\n",
    "        aw(self.node.unlock())\n",
    "        self.client.close()\n",
    "        \n",
    "        print(\"[Thymio] Disconnected\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0be53",
   "metadata": {},
   "source": [
    "The connection is established at the start of the navigation loop in [`main.py`](main.py):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6231a",
   "metadata": {},
   "source": [
    "## Global Navigation\n",
    "\n",
    "The global navigation system computes an optimal collision-free path from the robot's starting position to the goal before movement begins. It uses the map of static obstacles provided by the vision system to build a **visibility graph**: a network where nodes represent potential waypoints (vertices of the expanded obstacles, start, and goal), and edges connect pairs of nodes that have an unobstructed line of sight between them. The A* algorithm then searches this graph to find the shortest path, resulting in a sequence of waypoints the robot should follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5cc89",
   "metadata": {},
   "source": [
    "### Visibility Graph Construction\n",
    "\n",
    "Before constructing the visibility graph, the system first assembles a complete list of all nodes (waypoints) that will form the vertices of the graph. This node list is created by combining:\n",
    "\n",
    "1. **Start position**: The robot's current pose (index 0 in the node list)\n",
    "2. **Goal position**: The target destination (index 1 in the node list)  \n",
    "3. **Expanded obstacle vertices**: All corner points of the obstacles after they have been expanded by the robot's radius to account for its physical size\n",
    "\n",
    "This node assembly happens in the [`compute_path`](path_planner.py) method before calling the visibility graph builder:\n",
    "\n",
    "```python\n",
    "nodes = [np.array(start), np.array(goal)]\n",
    "for poly in obstacles:\n",
    "    for v in poly:\n",
    "        nodes.append(np.array(v))\n",
    "```\n",
    "\n",
    "By placing start and goal at indices 0 and 1 respectively, the pathfinding algorithm knows exactly which nodes represent the start and end points without needing to search through the entire list.\n",
    "\n",
    "Once the waypoint nodes are established, the system constructs a **visibility graph** that defines which waypoints can be directly connected by straight-line paths.\n",
    "\n",
    "The visibility graph is built by the [`build_visibility_graph`](path_planner.py) method, which operates as follows:\n",
    "\n",
    "1. **Graph Initialization**: An empty adjacency list is created for each node in the node list.\n",
    "\n",
    "2. **Pairwise Visibility Testing**: For every unique pair of nodes, the system checks if a straight line between them is obstacle-free using the [`_is_visible`](path_planner.py) method. This visibility check includes:\n",
    "   - **Special case handling**: If both nodes are vertices of the same polygon, the connection is only allowed if they are adjacent vertices (connected by an edge). This prevents diagonal connections that would cut through the obstacle's interior.\n",
    "   - **Edge intersection testing**: The line segment is tested against all obstacle edges to ensure it doesn't intersect with any boundaries. The intersection test uses an **orientation-based geometric algorithm**$^{[3.1]}$ that works as follows:\n",
    "     \n",
    "     To determine if line segment $p_1p_2$ intersects with obstacle edge $q_1q_2$, we compute the **orientation** of four triplets of points:\n",
    "     - Orientation of $(p_1, p_2, q_1)$: Is $q_1$ to the left, right, or on the line $p_1p_2$?\n",
    "     - Orientation of $(p_1, p_2, q_2)$: Is $q_2$ to the left, right, or on the line $p_1p_2$?\n",
    "     - Orientation of $(q_1, q_2, p_1)$: Is $p_1$ to the left, right, or on the line $q_1q_2$?\n",
    "     - Orientation of $(q_1, q_2, p_2)$: Is $p_2$ to the left, right, or on the line $q_1q_2$?\n",
    "     \n",
    "     The orientation is computed using the **cross product** of vectors: for points $(a, b, c)$, the orientation is determined by the sign of $(b_x - a_x)(c_y - a_y) - (b_y - a_y)(c_x - a_x)$. This value is positive if $c$ is to the left of line $\\overrightarrow{ab}$, negative if to the right, and zero if collinear.\n",
    "     \n",
    "     **Intersection occurs** when the two endpoints of each segment lie on **opposite sides** of the other segment's line. Specifically, if $(p_1, p_2, q_1)$ and $(p_1, p_2, q_2)$ have different orientations, **and** $(q_1, q_2, p_1)$ and $(q_1, q_2, p_2)$ have different orientations, then the segments intersect. Special handling is required for collinear cases where points lie on the same line.\n",
    "\n",
    "3. **Edge Creation**: If two nodes are mutually visible (pass both checks above), an edge is added to the graph **in both directions** with a weight equal to the Euclidean distance between them.\n",
    "\n",
    "The implementation in [`path_planner.py`](path_planner.py) demonstrates this process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c827f",
   "metadata": {},
   "source": [
    "ADD A BIT OF CODE ABOVE TO BE ABLE TO RUN THIS SUCH THAT IT PRODUCES A VISIBILITY GRAPH - Call the functions from path planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f746195",
   "metadata": {},
   "source": [
    "### Path Planning Algorithm\n",
    "\n",
    "We implemented the **A\\* (A-star) algorithm** to find the shortest path through the environment. A\\* was chosen over simpler alternatives like Dijkstra's algorithm because it uses a **heuristic** function—an estimated cost that predicts the remaining distance from any node to the goal—to guide the search more efficiently. While Dijkstra explores nodes uniformly in all directions, A\\* prioritizes nodes that appear closer to the goal based on this heuristic.$^{[3.2]}$ \n",
    "\n",
    "In our implementation, we use the **Euclidean distance** (straight-line distance) as the heuristic: $h(n) = \\sqrt{(x_n - x_{goal})^2 + (y_n - y_{goal})^2}$. This heuristic is admissible because it never overestimates the actual shortest path distance, which is a key requirement for A\\* to guarantee optimal solutions $^{[3.3]}$. This targeted exploration reduces computation time—a critical advantage for real-time robotics applications where the robot may need to replan paths quickly, such as after kidnapping detection. \n",
    "\n",
    "In our specific use case with only two quadrilateral obstacles (resulting in approximately 10 nodes total including start and goal), the computational difference between A\\* and Dijkstra's algorithm is negligible. However, A\\* was chosen for its scalability and to demonstrate best practices for path planning in more complex environments with numerous obstacles.\n",
    "\n",
    "The A\\* algorithm is implemented in the [`_a_star`](path_planner.py) method within the `PathPlanner` class, which searches the visibility graph using a priority queue to efficiently explore nodes in order of their estimated total cost to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310f40b",
   "metadata": {},
   "source": [
    "ADD CODE HERE TO SHOW THE SHORTEST PATH WHICH IS CALCULATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c7a71",
   "metadata": {},
   "source": [
    "### Output\n",
    "The result of the path planning is a sequence of waypoint coordinates $[(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)]$ that defines the optimal trajectory from start to goal. This path is then passed to the motion controller, which navigates the robot through each waypoint in sequence while the Kalman filter provides accurate pose estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f620e",
   "metadata": {},
   "source": [
    "## Control System Overview (à voir si on garde)\n",
    "\n",
    "### Add image and explanation of the control system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520517e",
   "metadata": {},
   "source": [
    "## Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a88b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a033d37e",
   "metadata": {},
   "source": [
    "## Motion Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9dae9e",
   "metadata": {},
   "source": [
    "The motion controller is responsible for computing wheel velocities that steer the robot toward each waypoint in the planned path. Our implementation is based on a **modified Astolfi controller**, adapted from the control law presented in the Basics of Mobile Robotics course.\n",
    "\n",
    "### The Standard Astolfi Controller\n",
    "\n",
    "The original Astolfi controller uses a polar coordinate representation to drive a differential-drive robot toward a target pose. It defines three error variables:\n",
    "- $\\rho$: the Euclidean distance to the target\n",
    "- $\\alpha$: the angle between the robot's heading and the direction to the target\n",
    "- $\\beta$: the angle of arrival (the desired final orientation at the target)\n",
    "\n",
    "### ADD IMAGE FROM COURSE - OR OUR OWN\n",
    "\n",
    "The control law then computes linear velocity $v$ and angular velocity $\\omega$ as functions of these three variables, ensuring the robot reaches the target with a specific orientation.\n",
    "\n",
    "### Our Modification: Ignoring $\\beta$\n",
    "\n",
    "In our application, we only care about **reaching each waypoint's position**, not about the robot's orientation upon arrival. Intermediate waypoints are simply locations the robot must pass through, and even at the final goal, the required orientation is not specified. Therefore, we **eliminate the $\\beta$ term entirely** from the controller, simplifying it to:\n",
    "\n",
    "$$\\rho = \\sqrt{(x_{target} - x_{robot})^2 + (y_{target} - y_{robot})^2}$$\n",
    "\n",
    "$$\\alpha = -\\theta_{robot} + \\text{atan2}(y_{target} - y_{robot}, x_{target} - x_{robot})$$\n",
    "\n",
    "$$v = k_\\rho \\cdot \\rho \\cdot \\cos(\\alpha)$$\n",
    "\n",
    "$$\\omega = k_\\alpha \\cdot \\alpha$$\n",
    "\n",
    "Where $k_\\rho$ and $k_\\alpha$ are proportional gains (in $s^{-1}$) that control how aggressively the robot reduces distance and corrects heading errors, respectively. The function $\\text{atan2}(y, x)$ is the two-argument arctangent, which computes the angle from the positive x-axis to the point $(x, y)$, correctly handling all four quadrants (unlike the standard $\\arctan$ which only returns values in $[-\\frac{\\pi}{2}, \\frac{\\pi}{2}]$). The resulting angle $\\alpha$ is then normalized to $[-\\pi, \\pi]$ to handle angle wraparound correctly.\n",
    "\n",
    "### Conversion to Wheel Speeds\n",
    "\n",
    "The computed linear velocity $v$ and angular velocity $\\omega$ are converted to the individual left and right wheel angular velocities using the differential drive kinematics:\n",
    "\n",
    "$$\\phi_{l} = \\frac{v + L \\cdot \\omega}{r}, \\quad \\phi_{r} = \\frac{v - L \\cdot \\omega}{r}$$\n",
    "\n",
    "Where $r$ is the wheel radius and $L$ is the half-width of the robot (distance from center to wheel). The speeds are then scaled if necessary to respect the Thymio's maximum motor speed while preserving the turning ratio.\n",
    "\n",
    "### Why this Controller?\n",
    "\n",
    "We chose this waypoint-based proportional controller rather than full trajectory-tracking methods—such as PID control or MPC—because it offers a good balance of simplicity, robustness, and computational efficiency. A pure PID controller is poorly suited for trajectory tracking on a differential-drive robot: the robot is nonlinear and nonholonomic, meaning it cannot directly correct lateral errors, yet PID implicitly assumes the system can reduce error in any direction. This mismatch often leads to oscillations or failure to converge. On the other hand, Model Predictive Control (MPC) is significantly more complex because it requires solving an optimization problem at every control step, taking into account a full motion model, prediction horizon, constraints, and numerical solver settings. This makes MPC computationally demanding and difficult to tune for real-time operation. In contrast, nonlinear controllers such as Astolfi’s law provide closed-form control inputs with minimal computation, making them considerably easier to implement\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The controller is implemented in the [`compute_speed`](motion_controller.py) method of the `MotionController` class. For now, we consider navigation without local obstacles—the next section on **Local Navigation** explains how this controller is modulated when unexpected obstacles are detected.\n",
    "\n",
    "The main navigation loop in [`main.py`](main.py) manages waypoint sequencing as follows:\n",
    "\n",
    "1. **Initialization**: After path planning, the `waypoint_idx` is set to 0 (the first waypoint after the start position).\n",
    "\n",
    "2. **Distance Check**: Each iteration, the system computes the Euclidean distance from the robot's current filtered position (from the Kalman filter) to the current waypoint.\n",
    "\n",
    "3. **Waypoint Transition**: When this distance falls below a threshold (`WAYPOINT_THRESHOLD = 30` pixels), the waypoint index is incremented and the robot begins navigating toward the next waypoint in the sequence.\n",
    "\n",
    "4. **Goal Detection**: When the robot reaches the final waypoint in the path (the goal) within the same 30-pixel threshold, navigation is complete. The motors are stopped and the program terminates.\n",
    "\n",
    "5. **Speed Commands**: At each control cycle, the wheel speeds are computed by the `compute_speed` method and then sent to the robot through the `set_speed` method.\n",
    "\n",
    "This simple proportional controller proved effective for our environment, providing smooth trajectories toward each waypoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537f441",
   "metadata": {},
   "source": [
    "## Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0a029",
   "metadata": {},
   "source": [
    "While global path planning provides an optimal collision-free trajectory based on known obstacles detected by the camera, the robot must also handle **unexpected obstacles** that appear during navigation. These dynamic obstacles are not visible in the camera frame and cannot be anticipated during the planning phase. To address this, we implemented a reactive local navigation system using the Thymio's infrared proximity sensors.\n",
    "\n",
    "### Artificial Neural Network (ANN) Approach\n",
    "\n",
    "The local obstacle avoidance is implemented using an **Artificial Neural Network (ANN)** that runs autonomously on the Thymio at 10Hz. This approach is inspired by Braitenberg vehicles and uses a simple single-layer neural network architecture.\n",
    "\n",
    "**Architecture:**\n",
    "- **Inputs**: 7 proximity sensor readings (5 front sensors + 2 rear sensors)\n",
    "- **Outputs**: 2 motor speeds (left and right wheels)\n",
    "- **Weights**: A 2×7 weight matrix that maps sensor activations to motor commands\n",
    "\n",
    "The weight matrix is designed such that:\n",
    "- Front center sensors strongly reduce both wheel speeds (braking)\n",
    "- Front left sensors reduce right wheel speed more than left (turn right to avoid)\n",
    "- Front right sensors reduce left wheel speed more than right (turn left to avoid)\n",
    "- Rear sensors provide a slight bias to help escape tight situations\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "The ANN program is compiled and uploaded to the Thymio using the Aseba scripting language. The code runs directly on the robot's microcontroller, ensuring fast reaction times without communication delays. The behavior can be summarized as:\n",
    "\n",
    "```\n",
    "For each proximity sensor reading:\n",
    "    motor_left += sensor_value * weight_left[sensor_index]\n",
    "    motor_right += sensor_value * weight_right[sensor_index]\n",
    "```\n",
    "\n",
    "The weights were tuned experimentally to achieve smooth avoidance behavior while maintaining forward progress toward the goal.\n",
    "\n",
    "### Integration with Global Navigation\n",
    "\n",
    "The local avoidance system is activated when the maximum proximity sensor reading exceeds a threshold (typically 1500-2000 on a scale of 0-4500). When no obstacles are nearby, the robot follows motor commands from the motion controller based on the global path. When obstacles are detected, the ANN modulates these base speeds to reactively avoid collisions while still attempting to maintain the general direction toward the waypoint.\n",
    "\n",
    "This hybrid approach combines the optimality of global planning with the reactivity of local avoidance, allowing the robot to navigate complex environments with both static and dynamic obstacles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871deb4a",
   "metadata": {},
   "source": [
    "## Kidnapping Resilience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e88ed3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52c0bf31",
   "metadata": {},
   "source": [
    "## Dead Reckoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861eaee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368a3433",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240f614",
   "metadata": {},
   "source": [
    "\"blabla\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b914c",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "**Thymio data**\n",
    "\n",
    "[1.1]\n",
    "\n",
    "**Vision**\n",
    "\n",
    "**Path Planning**\n",
    "\n",
    "[3.1] GeeksforGeeks, “Check if two given line segments intersect.” Accessed: Dec. 3, 2025. [Online]. Available: https://www.geeksforgeeks.org/check-if-two-given-line-segments-intersect/\n",
    "\n",
    "[3.2] Baeldung, “Dijkstra vs. A* – Pathfinding,” Baeldung on Computer Science, 2023.\n",
    "Available: https://www.baeldung.com/cs/dijkstra-vs-a-pathfinding\n",
    "\n",
    "[3.3] K. Kask, “Informed Heuristic Search,” CS 271: Introduction to Artificial Intelligence, University of California, Irvine. Accessed: Dec. 3, 2025. [Online]. Available: https://ics.uci.edu/~kkask/Fall-2016%20CS271/slides/03-InformedHeuristicSearch.pdf\n",
    "\n",
    "**Kalman Filter**\n",
    "\n",
    "**Controller**\n",
    "\n",
    "**Local Avoidance**\n",
    "\n",
    "**Dead Reckoning**\n",
    "\n",
    "**Other**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-course)",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
